# -*- coding: utf-8 -*-
"""crypto_index_construction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p78BLOm-XH6EJybK0ROImGfH6ESyxQLf

## Crypto Sentiment Index 2.0

Testing the implementation of the crypto sentiment index construction using Tensorflow 2.0 and the latest crypto messages from StockTwits.
"""

import os
import sys
import random
sys.path.append('/content/drive/MyDrive/crypto_index_construction')

import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt

from data_processor import *
from index_builder import *

"""Set parameters"""

embedding_dim = 100
max_length = 50
test_portion = .1
vocab_size = 10000
num_epochs = 10
train_from_scratch = False
# weights_path = "/content/drive/MyDrive/crypto_index_construction/best_weights.hdf5"
# data_path = "/content/drive/MyDrive/crypto_index_construction/messages_df.csv"
# embeddings_path = "/content/drive/MyDrive/crypto_index_construction/glove.6B.100d.txt"
# lexicon_path = "/content/drive/MyDrive/crypto_index_construction/l2_lexicon.csv"
weights_path = os.getcwd() + "/best_weights.hdf5"
data_path = os.getcwd() + "/messages_df.csv"
embeddings_path = os.getcwd() + "/glove.6B.100d.txt"
lexicon_path = os.getcwd() + "/l2_lexicon.csv"

"""Read and process data"""

messages_df = pd.read_csv(data_path, index_col=0)
messages_df = messages_df.rename(columns={'message': 'body'})

data_processor = DataProcessor(messages_df, vocabulary_size=vocab_size, min_msg_length=5,
                               max_msg_length=50, min_term_frequency=20)
all_seqs_padded, sentiments_filtered = data_processor.get_mapped_messages()
labels = [1 if x == 'Bullish' else 0 for x in sentiments_filtered]
word_index = data_processor.word_to_index_map

# ----- Reshuffle sequences and labels and collect them again
training_size = len(all_seqs_padded)
corpus = list(zip(all_seqs_padded, labels))
random.shuffle(corpus)
final_sequences = []
final_labels = []
for i in range(training_size):
    final_sequences.append(corpus[i][0])
    final_labels.append(corpus[i][1])

# ----- Get the final test and train data
split = int(test_portion * training_size)
test_sequences = final_sequences[0:split]
training_sequences = final_sequences[split:training_size]
test_labels = final_labels[0:split]
training_labels = final_labels[split:training_size]

training_padded = np.array(training_sequences)
training_labels = np.array(training_labels)
testing_padded = np.array(test_sequences)
testing_labels = np.array(test_labels)

print(training_size)
print(final_sequences[10])
print(final_labels[10])

"""Get the GloVe embedding matrix"""

embeddings_index = {}
with open(embeddings_path) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embeddings_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector

"""Set up and train the model using the optimal learning rate"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim,
                              input_length=max_length, weights=[embeddings_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

if train_from_scratch:
    checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=2, save_best_only=True, mode='min')
    history = model.fit(training_padded, training_labels, epochs=num_epochs,
                        validation_data=(testing_padded, testing_labels),
                        verbose=2, callbacks=[checkpoint])

"""Trained model diagnostics"""
if train_from_scratch:
    # ----- Retrieve results on training and test data
    #       sets for each training epoch
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(acc))  # get the number of epochs

    # ----- plot training and validation accuracy per epoch
    plt.plot(epochs, acc, 'r')
    plt.plot(epochs, val_acc, 'b')
    plt.title('Training and validation accuracy')
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend(["Accuracy", "Validation Accuracy"])
    plt.figure()

    # ----- plot training and validation loss per epoch
    plt.plot(epochs, loss, 'r')
    plt.plot(epochs, val_loss, 'b')
    plt.title('Training and validation loss')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend(["Loss", "Validation Loss"])
    plt.figure()

"""Load the best saved weights into the model"""

# ----- Load the best weights
model.load_weights(weights_path)
loss, acc = model.evaluate(testing_padded, testing_labels, verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100 * acc))

"""Create the index doing the following:


*   predict sentiment for unlabeled data
*   construct sentiment index time series from the classification results


"""
# ============================ Sentiment index construction ==============================
# ----- load data
seed_df = pd.read_csv(lexicon_path, sep=';')
messages_df_full = data_processor.messages_df_with_tokens

# ----- create an index builder object
index_builder = IndexBuilder(messages_df=messages_df_full, word2idx=data_processor.word_to_index_map,
                             w2v_matrix=embeddings_matrix, seed_df=seed_df,
                             min_msg_length=5, max_msg_length=50, rnn_model=model)

# ----- generate sentiments and scores
index_builder.add_classfn_sentiment()
index_builder.add_w2v_sentiment()
index_builder.add_classfn_score_1()
index_builder.add_classfn_score_2()


